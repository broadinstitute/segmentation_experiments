{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.load_all_data import load_data_by_color, load_imgs_masks\n",
    "from functions.load_training_data import load_rescaled_samples_opt, load_rescaled_samples\n",
    "from functions.crop_image import random_crop\n",
    "from functions.composites import composite_masks\n",
    "from functions.rescaling import rescale_img_comp\n",
    "from functions.sizes import compute_avg_size\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDataSet:\n",
    "    \"\"\"\n",
    "    Data structure containing randomly cropped images and\n",
    "    corresponding mask composites. Returns a tensor of \n",
    "    normalized images, their mask composites, and the corresponding\n",
    "    sizes of the nuclei in each image.\n",
    "    \n",
    "    Data is returned as a 2 dimensional array\n",
    "        [x][:] x is the index to the tuple containing the nuclei size, mask composite, and image\n",
    "        [:][x] x=0 --> nuclei size, x=1 --> max composite, x=2 --> image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_samples=1000, bins=50, crop_size=128, loud=False):\n",
    "        # NOTE: rescaling using load_rescaled_samples_opt may take ~20 minutes\n",
    "        # use load_rescaled_samples for faster results\n",
    "#         cropped_sizes, cropped_comps, cropped_imgs = load_rescaled_samples_opt(n_samples,\n",
    "#                                                                               BINS=bins,\n",
    "#                                                                               CROP_SIZE=crop_size,\n",
    "#                                                                                loud=loud)\n",
    "        \n",
    "        cropped_sizes, cropped_comps, cropped_imgs = load_rescaled_samples(n_samples)\n",
    "        self.cropped_sizes = cropped_sizes\n",
    "        self.cropped_comps = cropped_comps\n",
    "        self.cropped_imgs = cropped_imgs\n",
    "        self.ToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cropped_sizes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.cropped_sizes[idx], self.cropped_comps[idx], self.cropped_imgs[idx]\n",
    "    \n",
    "    def ToTensor(self):\n",
    "        \"\"\"\n",
    "        Convert data structure to a tensor for PyTorch manipulations\n",
    "            must swap axis because\n",
    "            numpy image: H x W x C\n",
    "            torch image: C x H x W\n",
    "        \"\"\"\n",
    "        for i in range(len(self.cropped_sizes)):\n",
    "            self.cropped_imgs[i] = self.cropped_imgs[i].transpose((2, 0, 1))\n",
    "            \n",
    "            self.cropped_comps[i] = torch.tensor(self.cropped_comps[i].astype(float) \n",
    "                                                                   / 255.).type(\"torch.FloatTensor\")\n",
    "            self.cropped_imgs[i] = torch.tensor(self.cropped_imgs[i].astype(float)).type(\"torch.FloatTensor\")\n",
    "            \n",
    "            self.cropped_sizes = list(self.cropped_sizes)\n",
    "            self.cropped_sizes = torch.LongTensor(self.cropped_sizes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SizeDataSet(loud=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = random_split(training_data, [int(0.8*len(training_data)), \n",
    "                                         int(0.1*len(training_data)),\n",
    "                                         int(0.1*len(training_data)),\n",
    "                                         ])\n",
    "train_set = DataLoader(split_data[0], batch_size=50)\n",
    "validation_set = DataLoader(split_data[1], batch_size=50)\n",
    "test_set = DataLoader(split_data[2], batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.moveaxis(split_data[0][1][2].numpy(), 0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU for training\n",
    "# check for available GPUs using \"watch nvidia-smi\" in terminal\n",
    "device=torch.device(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning Model\n",
    "* This is the same model as the Keras implementation\n",
    "* I moved this model to a pytorch lightning class in order to give it structure and aid readablibilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "criterion = nn.L1Loss() #equivalent to mean absolute error\n",
    "\n",
    "class LightningSizePredictor(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LightningSizePredictor, self).__init__()\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64,\n",
    "                                                   kernel_size=(3, 3), stride=(2,2)),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(in_channels=64, out_channels=32,\n",
    "                                                   kernel_size=(3, 3), stride=(2,2)),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(in_channels=32, out_channels=16,\n",
    "                                                   kernel_size=(3,3), stride=(2,2)),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Flatten(),\n",
    "                                          nn.Linear(392, 256),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(256, 1)\n",
    "                                         )\n",
    "        self.train_regress = nn.L1Loss()\n",
    "        self.valid_regress = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.size_train, self.size_val = train_set, validation_set\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_reg = self.train_regress(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_loss_step', train_reg, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log('train_reg_epoch', self.train_regress.compute())\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.size_train, batch_size=64)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.size_val, batch_size=64)\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self(x)\n",
    "        loss = criterion(logits, y)\n",
    "        val_reg = self.valid_acc(logits, y)\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('valid_reg', val_reg, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = nn.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss':avg_loss, 'log':tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "model = LightningSizePredictor()\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                     dirpath='checkpoints/',\n",
    "                                     filename='sizeMod-{epoch:03d}-{val_loss:.2f}-{valid_acc:.2f}',\n",
    "                                     save_top_k=3, \n",
    "                                     mode='min')\n",
    "\n",
    "# be sure to change the gpus parameter to match the reserved gpu from above\n",
    "trainer = pl.Trainer(gpus='7',\n",
    "                    max_epochs=10,\n",
    "                    default_root_dir='checkpoints/', checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
